{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. WAP to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function.\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1, epochs=10000):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) \n",
    "        self.b1 = np.zeros((1, hidden_size))              \n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))  \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        output_error = y - self.a2\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.a2)\n",
    "\n",
    "        hidden_error = output_delta.dot(self.W2.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.a1)\n",
    "\n",
    "        self.W2 += self.a1.T.dot(output_delta) * self.learning_rate\n",
    "        self.b2 += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.W1 += X.T.dot(hidden_delta) * self.learning_rate\n",
    "        self.b1 += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y) \n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean(np.square(y - self.a2))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR Gate\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2   \n",
    "hidden_size = 4  \n",
    "output_size = 1  \n",
    "\n",
    "mlp = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, learning_rate=0.1, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.00225393489829456\n",
      "Epoch 1000, Loss: 0.0019442206323379706\n",
      "Epoch 2000, Loss: 0.0017057789411882608\n",
      "Epoch 3000, Loss: 0.0015170525038492107\n",
      "Epoch 4000, Loss: 0.00136427224842055\n",
      "Epoch 5000, Loss: 0.0012382616820502703\n",
      "Epoch 6000, Loss: 0.0011326855222450582\n",
      "Epoch 7000, Loss: 0.00104303807793501\n",
      "Epoch 8000, Loss: 0.0009660325334299373\n",
      "Epoch 9000, Loss: 0.000899218116024192\n",
      "\n",
      "Predictions after training:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01476871],\n",
       "       [0.97020023],\n",
       "       [0.97061914],\n",
       "       [0.03733032]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network\n",
    "mlp.train(X, y)\n",
    "\n",
    "# Test the network\n",
    "print(\"\\nPredictions after training:\")\n",
    "predictions = mlp.predict(X)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = (predictions > 0.5).astype(int)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Explanation:\n",
    "The network was trained for 10,000 epochs using the XOR dataset.\n",
    "The loss gradually decreased as the network learned the XOR function.\n",
    "The output predictions after training are close to [0, 1, 1, 0], which is the expected output for the XOR truth table.\n",
    "Tuning Parameters:\n",
    "You can experiment with the hidden_size (number of neurons in the hidden layer) and learning_rate to improve performance and convergence speed.\n",
    "The epochs value controls how many times the entire dataset is passed through the network during training. If necessary, you can adjust this number to reach a lower loss.\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
